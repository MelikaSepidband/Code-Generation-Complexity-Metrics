# Code-Generation-Complexity-Metrics

This project explores the use of code complexity metrics as feedback to enhance the accuracy and correctness of code generated by Large Language Models (LLMs) like GPT-4o. While LLMs have significantly advanced automatic code generation, they still face challenges in producing code that passes all test cases and aligns with desired complexity levels.

# Objectives

Our main goal is to guide LLMs toward generating correct code by understanding and utilizing the expected complexity characteristics of the desired solutions. We achieve this by:

Analyzing Complexity Metrics: We employ 53 widely-used complexity metrics—including Halstead complexity measures and Cyclomatic complexity—to quantify the inherent complexity of the generated code.

Establishing Correlation: We investigate the correlation between these complexity metrics and the success rate of code generation (measured by Pass@1).

Iterative Feedback Method: We introduce an iterative method that provides feedback to the LLM based on the most impactful complexity metrics, prompting it to adjust and refine the code accordingly. You can see the overiview of our method in this figure:

![Overview of Complexity-Aware Feedback for Enhanced LLM Code Generation](https://github.com/user-attachments/assets/111c65ee-f071-4d34-aa99-9b03a4996606)


# Key Findings

Correlation with Pass@1: There is a clear correlation between specific complexity metrics and the effectiveness of LLM-generated code. Using logistic regression, we observed that certain metrics are strong predictors of whether the generated code will pass all test cases.

Distribution Differences: By analyzing correct and incorrect code samples, we identified patterns in complexity metric distributions that distinguish successful code generations from failed ones.

Improved Code Generation: Our iterative feedback method significantly improves the correctness of generated code. For example, using GPT-3.5 Turbo on the HumanEval dataset, our approach increased Pass@1 by 35.71%, compared to a baseline improvement of 12.5%.

# Contributions
Demonstrated the Correlation: Showed a significant correlation between code complexity metrics and the success of code generation by LLMs.

Introduced a Novel Method: Developed an iterative feedback mechanism based on complexity metrics to guide LLMs in producing correct code.

Comprehensive Experiments: Validated our approach through extensive experiments across multiple datasets (HumanEval, MBPP-sanitized, LeetCode) and LLMs (GPT-4o, GPT-3.5 Turbo, Llama 3.1).

# Conclusion
By leveraging code complexity metrics as feedback, we can effectively guide LLMs to generate more accurate and efficient code. This approach not only enhances the success rate of code generation but also contributes to the development of more reliable and robust AI coding assistants.
